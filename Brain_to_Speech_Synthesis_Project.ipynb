{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN6oP87yGfYV"
      },
      "source": [
        "# Milestone 1: Data acquisition, Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2UfNu8tGZVE"
      },
      "source": [
        "# 1. Data Source, Installation and Data Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28Gy7jYzIw-2"
      },
      "source": [
        "The data that will be used in the development of this project is the [Dataset of Speech Production in intracranial Electroencephalography](https://www.nature.com/articles/s41597-022-01542-9), this dataset can be downloaded from [here](https://osf.io/nrgx6/download) and then was uploaded to google drive.\n",
        "\n",
        "The dataset is based on 10 participants reading out individual words while being measured his intracranial EEG from a total of 1103 electrodes. It has a high temporal resolution and coverage of a large variety of cortical and sub-cortical brain regions, can help in understanding the speech production process better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InFIYquDLYVV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45cfaa24-303c-4bd8-9782-dd2b0a5290e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.0/134.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m55.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.9/785.9 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.7/727.7 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.5/331.5 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.9/112.9 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install every library that we will need for the development of the project\n",
        "\n",
        "!pip install numpy scipy scikit-learn pandas pynwb nilearn nibabel RutishauserLabtoNWB pytorch-lightning --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1L3OlkmLYrZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import pynwb\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from nilearn import plotting\n",
        "import RutishauserLabtoNWB as RLab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdRw_Ld8TKeL"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "from torchmetrics import Accuracy\n",
        "\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBe3WAIpLba6",
        "outputId": "d0ed44c4-a4c4-4981-c9d8-c21f597f24fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Give permission to acces Google Drive cause there is where the Zip File is\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUhf9fEmLnlU",
        "outputId": "ef5f13c8-7f68-4ba1-896e-17b76a4372e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Colab Notebooks'\n"
          ]
        }
      ],
      "source": [
        "!ls \"/content/drive/My Drive/\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys5Wq0kvPyzO",
        "outputId": "2fc14a74-3c2f-46ce-d043-9a35f4bf7ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open /content/drive/My Drive/SingleWordProductionDutch-iBIDS.zip, /content/drive/My Drive/SingleWordProductionDutch-iBIDS.zip.zip or /content/drive/My Drive/SingleWordProductionDutch-iBIDS.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "# Let's unzip the file\n",
        "!unzip -o  \"/content/drive/My Drive/SingleWordProductionDutch-iBIDS.zip\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV6tlFYFUshh",
        "outputId": "f1ef083d-7221-404a-8949-255e643a52a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  sample_data\n"
          ]
        }
      ],
      "source": [
        "# Let's check if the file is already unziped\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWIFICSWUz5J",
        "outputId": "e61d9780-9f73-4432-8694-326e832f92f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'SingleWordProductionDutch-iBIDS': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# Let's lists the files and directories in the current directory\n",
        "!ls \"SingleWordProductionDutch-iBIDS\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za221np6NoNo"
      },
      "source": [
        "## 1.2  We Clone the repository with the Scripts, so we can work with the intracranial EEG data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMty3J0dNXKu",
        "outputId": "77697d94-7b98-4082-85d0-1beed45875be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'SingleWordProductionDutch'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (65/65), done.\u001b[K\n",
            "remote: Total 86 (delta 47), reused 52 (delta 19), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (86/86), 21.48 KiB | 5.37 MiB/s, done.\n",
            "Resolving deltas: 100% (47/47), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/neuralinterfacinglab/SingleWordProductionDutch.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YT1lk9jbcLF"
      },
      "source": [
        "# 2. Data Exploration and Visualization\n",
        "\n",
        "Our dataset has a structure that follows the BIDS (Brain Imaging Data Structure) format, which is a standard in organizing neuroimaging and neurophysiology data.\n",
        "\n",
        "So we'll approach the data in the next way:\n",
        "1. Metadata Exploration\n",
        "2. Individual Participant Data\n",
        "3. Derivatives Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT6p9jK2cAu5"
      },
      "source": [
        "## 2.1. Metadata Exploration\n",
        "The root folder contains:\n",
        "- metadata of the participants (participants.tsv)\n",
        "- subject specific data folders (i.e., sub-01)\n",
        "- derivatives folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2KfdpUdOQy8"
      },
      "source": [
        "### 2.1.1 README"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "e45i-0X4VJxU",
        "outputId": "e454c3f6-fc56-4784-f2d3-fd1869d25f1e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-6d0e408c3e8e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read the README in case some important information is needed for the dataset evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SingleWordProductionDutch-iBIDS/README\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mreadme_contents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SingleWordProductionDutch-iBIDS/README'"
          ]
        }
      ],
      "source": [
        "# Read the README in case some important information is needed for the dataset evaluation\n",
        "\n",
        "with open(\"SingleWordProductionDutch-iBIDS/README\", \"r\") as file:\n",
        "    readme_contents = file.read()\n",
        "\n",
        "print(readme_contents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgFG_le0OUzt"
      },
      "source": [
        "### 2.1.2 Dataset Description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "CTWnXMmBVYqa",
        "outputId": "959d662f-f80c-4f18-969a-5c96e08d51f4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-ba533f50761e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Read the dataset description as an informative\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SingleWordProductionDutch-iBIDS/dataset_description.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mdataset_description\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SingleWordProductionDutch-iBIDS/dataset_description.json'"
          ]
        }
      ],
      "source": [
        "# Read the dataset description as an informative\n",
        "import json\n",
        "with open(\"SingleWordProductionDutch-iBIDS/dataset_description.json\", \"r\") as file:\n",
        "    dataset_description = json.load(file)\n",
        "\n",
        "# Display the contents of the JSON file\n",
        "dataset_description\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPTDGuErObGP"
      },
      "source": [
        "### 2.1.3 Participants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "id": "hVe0KUlIVl9d",
        "outputId": "032276b2-948a-462e-d5d6-754a31703db2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-0709a6d3fcb7>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Read and display participants.tsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mparticipants_tsv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SingleWordProductionDutch-iBIDS/participants.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mparticipants_tsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SingleWordProductionDutch-iBIDS/participants.tsv'"
          ]
        }
      ],
      "source": [
        "# Read for general demographic information of participants\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Read and display participants.tsv\n",
        "participants_tsv = pd.read_csv(\"SingleWordProductionDutch-iBIDS/participants.tsv\", sep='\\t')\n",
        "participants_tsv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "id": "N2F8ghtDWU9T",
        "outputId": "ad73fda1-b5ad-47b3-a370-52aa125f389e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-6017ef603f7f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Open and read participants.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SingleWordProductionDutch-iBIDS/participants.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mparticipants_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'SingleWordProductionDutch-iBIDS/participants.json'"
          ]
        }
      ],
      "source": [
        "# Read to understand the metadata of \"participants.tsv\"\n",
        "\n",
        "# Open and read participants.json\n",
        "with open(\"SingleWordProductionDutch-iBIDS/participants.json\", \"r\") as file:\n",
        "    participants_json = json.load(file)\n",
        "\n",
        "participants_json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn4abHFbW9jz"
      },
      "outputs": [],
      "source": [
        "# Create a dataframe of the dictionary for better understanding\n",
        "\n",
        "# Convert nested dictionary to a list of dictionaries for creating a DataFrame\n",
        "participants_json_list = []\n",
        "for key, value in participants_json.items():\n",
        "    row = {'key': key}\n",
        "    row.update(value)\n",
        "    participants_json_list.append(row)\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(participants_json_list)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqgJA4fMOpSA"
      },
      "source": [
        "### 2.1.4 Derivatives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXAH-LK9Wr7t"
      },
      "outputs": [],
      "source": [
        "!ls \"SingleWordProductionDutch-iBIDS/derivatives\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whTHtZ-xdKks"
      },
      "source": [
        "## 2.2 Individual Participant Data\n",
        "The subject specific folders contain .tsv files with information about:\n",
        "- the implanted electrode coordinates (_electrodes.tsv)\n",
        "- recording montage (_channels.tsv)\n",
        "- event markers (_events.tsv)\n",
        "- The _ieeg.nwb file contains three raw data streams as timeseries (iEEG, Audio and Stimulus), which are located in the acquisition container.\n",
        "- Descriptions of recording aspects and of specific .tsv columns are provided in correspondingly named .json files (i.e., participants.json).\n",
        "\n",
        "We will choose one participant for the development of the project, and with his data we will realize the training, validation and testing for the set of a single speaker.\n",
        "\n",
        "The election of the individual participant will be random, so the selected participant is **sub-01**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fjRQ6D3PKUt"
      },
      "source": [
        "### 2.2.1 The implanted electrode coordinates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUuE0gK7fkXR"
      },
      "outputs": [],
      "source": [
        "# 1. Load the electrodes.tsv for sub-01\n",
        "electrodes_tsv_path = \"/content/SingleWordProductionDutch-iBIDS/sub-01/ieeg/sub-01_task-wordProduction_space-ACPC_electrodes.tsv\"\n",
        "electrodes_data = pd.read_csv(electrodes_tsv_path, sep='\\t')\n",
        "display(electrodes_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcIaz7CQVZtf"
      },
      "outputs": [],
      "source": [
        "def value_plot(df, columns, figscale=1):\n",
        "    for col in columns:\n",
        "        df[col].plot(kind='line', title='Values of electrodes in sub-01',figsize=(5*figscale, 2.5*figscale), label=col)\n",
        "\n",
        "    plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "electrodes_chart = value_plot(electrodes_data, ['x', 'y', 'z'])\n",
        "display (electrodes_chart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqEEj4TXW3V3"
      },
      "outputs": [],
      "source": [
        "def histogram(df, columns, num_bins=20, figscale=1):\n",
        "    for col in columns:\n",
        "        df[col].plot(kind='hist', bins=num_bins, title=\"Distribution of electrodes in sub-01\",figsize=(5*figscale, 2.5*figscale), alpha=0.5, label=col)\n",
        "\n",
        "    plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "electrodes_distributionchart = histogram(electrodes_data, ['x', 'y', 'z'])\n",
        "display(electrodes_distributionchart)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba68fATVPStO"
      },
      "source": [
        "### 2.2.2 Recording montage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo2oXX_dgMIO"
      },
      "outputs": [],
      "source": [
        "# 2. Load the channels.tsv for sub-01\n",
        "channels_tsv_path = \"/content/SingleWordProductionDutch-iBIDS/sub-01/ieeg/sub-01_task-wordProduction_channels.tsv\"\n",
        "channels_data = pd.read_csv(channels_tsv_path, sep='\\t')\n",
        "display(channels_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezMW_3ikPXP_"
      },
      "source": [
        "### 2.2.3 Event markers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pX2Xjf9EgHzD"
      },
      "outputs": [],
      "source": [
        "# 3. Load the events.tsv for sub-01\n",
        "events_tsv_path = \"/content/SingleWordProductionDutch-iBIDS/sub-01/ieeg/sub-01_task-wordProduction_events.tsv\"\n",
        "events_data = pd.read_csv(events_tsv_path, sep='\\t')\n",
        "display(events_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQeYJrvtPbx4"
      },
      "source": [
        "### 2.2.4 The _ieeg.nwb file (iEEG, Audio and Stimulus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAgE1UZhfo0b"
      },
      "outputs": [],
      "source": [
        "# 4. Load the .nwb file\n",
        "# We use NWBHDF5IO to read the data stored in NWB files, accessing and analyzinge the neurophysiological data inside\n",
        "from pynwb import NWBHDF5IO\n",
        "\n",
        "ieeg_nwb_path = \"/content/SingleWordProductionDutch-iBIDS/sub-01/ieeg/sub-01_task-wordProduction_ieeg.nwb\"\n",
        "with NWBHDF5IO(ieeg_nwb_path, 'r') as io:\n",
        "  nwbfile = io.read()\n",
        "\n",
        "  # List the names of all data interfaces in the file\n",
        "  print(nwbfile.acquisition)\n",
        "\n",
        "  # Extract data for each interface\n",
        "  audio_data_sample = nwbfile.acquisition['Audio'].data[:15]\n",
        "  stimulus_data_sample = nwbfile.acquisition['Stimulus'].data[:15]\n",
        "  ieeg_data_sample = nwbfile.acquisition['iEEG'].data[:15]\n",
        "\n",
        "  print(\"Audio data:\", audio_data_sample)\n",
        "  print()\n",
        "  print(\"Stimulus data:\", stimulus_data_sample)\n",
        "  print()\n",
        "  print(\"iEEG data (first 5 channels):\", ieeg_data_sample[:, :5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUYSpOF6gtAF"
      },
      "outputs": [],
      "source": [
        "import h5py #HDF5 is designed to store and organize large amounts of numerical data\n",
        "\n",
        "with h5py.File(ieeg_nwb_path, 'r') as nwbfile:\n",
        "\n",
        "# Print the root-level keys in the HDF5 file\n",
        "  print(list(nwbfile.keys()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hf9OqBie-Gk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t40jzQqMiFXN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# Open the .nwb file\n",
        "path_to_file = \"/content/SingleWordProductionDutch-iBIDS/sub-01/ieeg/sub-01_task-wordProduction_ieeg.nwb\"\n",
        "with NWBHDF5IO(path_to_file, 'r') as io:\n",
        "  nwbfile = io.read()\n",
        "\n",
        "\n",
        "  audio_data_sample = nwbfile.acquisition['Audio'].data[:10]\n",
        "  stimulus_data_sample = nwbfile.acquisition['Stimulus'].data[:10]\n",
        "  ieeg_data_sample = nwbfile.acquisition['iEEG'].data[:10]\n",
        "\n",
        "print(\"Audio data:\", audio_data_sample)\n",
        "print(\"Stimulus data:\", stimulus_data_sample)\n",
        "print(\"iEEG data (first 5 channels):\", ieeg_data_sample[:, :5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ersSotmndaI"
      },
      "source": [
        "### 2.2.5 Descriptions of recording aspects and of specific .tsv columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgJXE11ZnhBU"
      },
      "outputs": [],
      "source": [
        "# 5. Load the first JSON file\n",
        "\n",
        "import json\n",
        "\n",
        "# Specify the path to the JSON file\n",
        "path_to_json = \"/content/SingleWordProductionDutch-iBIDS/sub-01/ieeg/sub-01_task-wordProduction_space-ACPC_coordsystem.json\"\n",
        "\n",
        "# Load the JSON file\n",
        "with open(path_to_json, 'r') as json_file:\n",
        "    data_description = json.load(json_file)\n",
        "\n",
        "# Print the contents\n",
        "print(json.dumps(data_description, indent = 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNXPKRF0poTh"
      },
      "outputs": [],
      "source": [
        "# 5. Load the second JSON file\n",
        "\n",
        "# Define the path to the JSON file\n",
        "file_path = '/content/SingleWordProductionDutch-iBIDS/sub-01/ieeg/sub-01_task-wordProduction_ieeg.json'\n",
        "\n",
        "# Open and load the JSON data\n",
        "with open(file_path, 'r') as json_file:\n",
        "    data = json.load(json_file)\n",
        "\n",
        "# Print the contents of the JSON file\n",
        "print(json.dumps(data, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE-6Yn7Boyw3"
      },
      "source": [
        "## 2.3 Derivatives Data\n",
        "The derivatives folder contains:\n",
        "- the pial surface cortical meshes of the right (_rh_pial.mat) and left (_lh_pial.mat) hemisphere\n",
        "- the brain anatomy (_brain.mgz)\n",
        "- the Destrieux atlas (_aparc.a2009s + aseg.mgz)\n",
        "- a white matter atlas (_wmparc.mgz) per subject, derived from the Freesurfer pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ddnb_WQ9Pdy9"
      },
      "outputs": [],
      "source": [
        "!ls SingleWordProductionDutch-iBIDS/derivatives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ksZNxBOKzu7"
      },
      "source": [
        "### 2.3.1 Pial Surface Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpuMd5cxCabB"
      },
      "outputs": [],
      "source": [
        "# Explore Pial Surface Data\n",
        "import scipy.io\n",
        "\n",
        "rh_pial_path = \"/content/SingleWordProductionDutch-iBIDS/derivatives/sub-01/sub-01_rh_pial.mat\"\n",
        "lh_pial_path = \"/content/SingleWordProductionDutch-iBIDS/derivatives/sub-01/sub-01_lh_pial.mat\"\n",
        "\n",
        "rh_pial = scipy.io.loadmat(rh_pial_path)\n",
        "lh_pial = scipy.io.loadmat(lh_pial_path)\n",
        "\n",
        "# Let's inspect the keys and structure of the loaded data\n",
        "print(rh_pial.keys())\n",
        "print(lh_pial.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhlVzC0cK1_C"
      },
      "source": [
        "### 2.3.2 Brain Anatomy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5iRuD5iC_eQ"
      },
      "outputs": [],
      "source": [
        "# Explore Brain Anatomy\n",
        "\n",
        "brain_data_path = \"/content/SingleWordProductionDutch-iBIDS/derivatives/sub-01/sub-01_brain.mgz\"\n",
        "brain_data = nib.load(brain_data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i30ALmITFc3C"
      },
      "outputs": [],
      "source": [
        "# Display the shape of the data\n",
        "print(\"Data shape:\", brain_data.shape)\n",
        "\n",
        "# Display header information\n",
        "print(brain_data.header)\n",
        "\n",
        "# Get the actual data as a numpy array (if needed)\n",
        "brain_numpy_data = brain_data.get_fdata()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ID1DmdbD1TC"
      },
      "outputs": [],
      "source": [
        "# Extract a 2D slice\n",
        "axial_slice = brain_numpy_data[:, :, brain_numpy_data.shape[2] // 2]\n",
        "\n",
        "plt.imshow(axial_slice.T, cmap=\"Blues\", origin=\"lower\")\n",
        "plt.title(\"Axial Lower Slice\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DyDJfEtsGIXS"
      },
      "outputs": [],
      "source": [
        "# Extract a 2D slice\n",
        "axial_slice = brain_numpy_data[:, :, brain_numpy_data.shape[2] // 2]\n",
        "\n",
        "plt.imshow(axial_slice.T, cmap=\"gray\", origin=\"upper\")\n",
        "plt.title(\"Axial Upper Slice\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu6Upe5dK8lu"
      },
      "source": [
        "### 2.3.3 The Destrieux atlas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgwdoYGhK8K8"
      },
      "outputs": [],
      "source": [
        "destrieux_atlas_path = \"/content/SingleWordProductionDutch-iBIDS/derivatives/sub-01/sub-01_aparc.a2009s+aseg.mgz\"\n",
        "destrieux_atlas_data = nib.load(destrieux_atlas_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BuLPkCvGLb68"
      },
      "outputs": [],
      "source": [
        "# Get the data array from the atlas\n",
        "atlas_array = destrieux_atlas_data.get_fdata()\n",
        "\n",
        "# Print the shape of the data to understand its dimensions\n",
        "print(atlas_array.shape)\n",
        "\n",
        "# Print header information to understand metadata\n",
        "print(destrieux_atlas_data.header)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6xl8YxQLdtI"
      },
      "outputs": [],
      "source": [
        "# Convert the data to a 3D numpy array\n",
        "atlas_img = np.asarray(atlas_array, dtype=np.int32)\n",
        "\n",
        "# Display the atlas using nilearn's plotting function\n",
        "plotting.plot_roi(destrieux_atlas_data, draw_cross=False, title=\"Destrieux Atlas\")\n",
        "plotting.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr1UmoXTMW8J"
      },
      "source": [
        "### 2.3.4  A white matter atlas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7P3hkcTMS1O"
      },
      "outputs": [],
      "source": [
        "import nibabel as nib\n",
        "\n",
        "wm_atlas_path = \"/content/SingleWordProductionDutch-iBIDS/derivatives/sub-01/sub-01_wmparc.mgz\"\n",
        "wm_atlas_data = nib.load(wm_atlas_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgqGt-LlMp5F"
      },
      "outputs": [],
      "source": [
        "# Extract data array from the atlas\n",
        "wm_array = wm_atlas_data.get_fdata()\n",
        "\n",
        "# Print the shape of the data\n",
        "print(wm_array.shape)\n",
        "\n",
        "# Print header information for metadata understanding\n",
        "print(wm_atlas_data.header)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUQujzROMsQy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from nilearn import plotting\n",
        "\n",
        "# Convert the data to a 3D numpy array\n",
        "wm_img = np.asarray(wm_array, dtype=np.int32)\n",
        "\n",
        "# Display the atlas using nilearn's plotting function\n",
        "plotting.plot_roi(wm_atlas_data, draw_cross=False, title=\"White Matter Atlas\")\n",
        "plotting.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqe_nJT7I0wJ"
      },
      "source": [
        "# 3. Preparing data for training\n",
        "- As we mention before, we filter the data so we are only using a single speaker sub-01."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iO6c-CGmIQn"
      },
      "source": [
        "Given the content of the NWBFile, there are three types of data under de acquisition field:\n",
        "- Audio <class 'pynwb.base.TimeSeries'>\n",
        "- Stimulus <class 'pynwb.base.TimeSeries'>\n",
        "- iEEG <class 'pynwb.base.TimeSeries'>\n",
        "\n",
        "iEEG stands for intracranial electroencephalography. It is a type of electroencephalography (EEG) where electrodes are placed directly on the exposed surface of the brain to record electrical activity. This is in contrast to traditional EEG where electrodes are placed on the scalp.\n",
        "\n",
        "iEEG data is particularly valuable. Speech production involves multiple regions of the brain, including the motor cortex, Broca's area, and others. The high spatial resolution of iEEG allows for the nuanced study of how these regions interact during the task. This makes it an essential dataset for understanding brain mechanisms involved in speech, which can be of significance in our project.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AyvMxkWhAeI"
      },
      "source": [
        "# 3.1 Preparing audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIbI8Cg0herP"
      },
      "outputs": [],
      "source": [
        "audio_data_np = np.array(audio_data_sample)\n",
        "max_amplitude = np.max(np.abs(audio_data_np))\n",
        "audio_data_normalized = audio_data_np / max_amplitude\n",
        "audio_tensor = torch.tensor(audio_data_normalized, dtype=torch.float32)\n",
        "batch_size = 1  # We might adjust the batch size\n",
        "audio_tensor = audio_tensor.view(batch_size, -1)\n",
        "audio_array = audio_tensor.squeeze().numpy()  # Squeeze removes dimensions of size 1 (in case batch_size is 1)\n",
        "# Plot the audio waveform\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.plot(audio_array)\n",
        "plt.xlabel('Sample')\n",
        "plt.ylabel('Amplitude')\n",
        "plt.title('Audio Waveform')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZJnAmQLlIPO"
      },
      "source": [
        "# 3.2 Preparing iEEG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OfpaAT3OlLK7"
      },
      "outputs": [],
      "source": [
        "ieeg_data_np = np.array(ieeg_data_sample)\n",
        "max_amplitude_ieeg = np.max(np.abs(ieeg_data_np))\n",
        "ieeg_data_normalized = ieeg_data_np / max_amplitude_ieeg\n",
        "ieeg_tensor = torch.tensor(ieeg_data_normalized, dtype=torch.float32)\n",
        "# Visualize iEEG data\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(ieeg_tensor.T, aspect='auto', cmap='viridis', origin='lower')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Channel')\n",
        "plt.title('iEEG Data Visualization')\n",
        "plt.colorbar(label='Amplitude')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEqvuPdPlMwb"
      },
      "source": [
        "#?splitting?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1Jla3MnlSr3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "ieeg_train, ieeg_temp = train_test_split(ieeg_tensor, test_size=0.2, random_state=42)\n",
        "ieeg_val, ieeg_test = train_test_split(X_temp, test_size=0.5, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Rn_MY38FraV"
      },
      "source": [
        "3.3 Data Exploration\n",
        " - Statistical Analysis\n",
        "\n",
        "3.4 Data Cleaning\n",
        "- Should we do the missing values, remove outliers, etc.\n",
        "3.5 Data Transformation\n",
        "\n",
        "3.6 Data Augmentation (should we use the audio or stimulus)\n",
        "\n",
        "3.7 Feature Extraction\n",
        "\n",
        "3.8 Data Splitting\n",
        "\n",
        "3.9 Data Serialization\n",
        "\n",
        "3.10 Data Pipeline Creation\n",
        "\n",
        "4. Final output: training, validation and test inputs and outputs.\n",
        "\n",
        "  4.1 Training\n",
        "\n",
        "  4.2 Validation\n",
        "\n",
        "  4.3 Test inputs and outputs"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}