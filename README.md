# DeepLearning_BME

# Brain-to-speech synthesis

> The purpose of a brain-computer interface (BCI) is to provide a natural or near-natural channel of communication for people who cannot speak due to physical or neurological impairment.
> Speech is the primary and most essential means of human communication. However, many people have lost this ability through illness or ill health. The real-time synthesis of speech directly from measured neural activity (BRAIN2SPEECH) would enable natural speech and significantly improve quality of life, especially for severely communication-impaired individuals.
> As students, we will study the BRAIN2SPEECH domain and then develop and train new types of neural network architectures.
> The data implemented in the project is the **Dataset of Speech Production in intracranial Electroencephalography** (SingleWordProductionDutch), available [here](https://osf.io/nrgx6/) .
> The related github repository contains a linear regression model which should be replaced by deep neural networks.
> Basic task: train/valid/test set from single speaker.
